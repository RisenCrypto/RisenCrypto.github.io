---
layout: post
mathjax: true
title: Few questions answered about PLONK
---

{% include mathjax.html %}

---- 

#### Copy Constraints
**Q:** Why are Copy Constraints required in PLONK unlike older SNARKs?

**A:** Older SNARKs like [Groth16](/R1CSQAP/), Pinocchio etc only verify the computation in each of the gates but do not have have something like Copy Constraints which verify the consistency across the cates. This is because the older SNARKs use a Witness/Solution vector in verifying computation in each of the gates. This Solution vector is common across all the gates. This automatically ensures the consistency across the gates which is what the Copy Constrainsts in PLONK is used to verify. 

--- 
#### Blinding
**Q:** In Round 1 (Page 28 of the PLONK paper), random blinding scalars are used to modify the 3 wire polynomials (the polynomials representing the left, right & output of the gates). What exactly is blinding?

**A:** Let's say you have a polynomial $f(x)$ of degree $d$ & it's commitment $C_f$. Let's say the verifier selects random value $r$ & the prover sends the evaluation of $f$ at $r$ i.e. $f(r) = z$

A polynomial of degree $d$ can be recreated with $d+1$ evaluations by using Lagrange Interpolation. So each opening of a Polynomial Commitment leaks some info about the polynomial. 

So SNARKs use a trick called blinding to make it zero knowledge. Multiply the vanishing polynomial $Z_H$ by a random Polynomial $R(x)$ & add it to $f(x)$ to create a new polynomial $F(x)$

$F(x) = R(x)\star Z_H + f(x)$

Instead of commiting and opening $f(x)$, the prover commits & opens $F(x)$. $Z_H$ is zero on the set the contrainsts are checked on - so on this set $F(x) = f(x)$

So other than in the commitment & opening, the SNARK can continue to use $f$ instead of $F$

The next question is what should be the degree of the random polynomial $R$. That depends on how many points you open $f$ at - if there is an opening at only one point, then $R$ needs to be of minimum degree $1$, if there are two openings, then there $R$ needs to be atleast of degree $2$. The new polynomial $F(x)$ which is going to be opened instead of $f(x)$ has to be of degree of $f$ plus the number of points at which $f$ needs to be opened.

In Round 1, the left, right & output polynomials are evaluated only at one point each & hence a random polynomial of degree $1$ is used for blinding - for e.g. for blinding the opening of $a(X)$, $R(x) = (b_1 X + b_2)$ is used as the random polynomial. The round 2 polynomial $z(X)$ is evaluated at 2 points & hence a degree 2 random polynomial $(b_7 X + b_8 X + b_9)$ is used.  

---- 
#### Linear Independence
**Q:** In Round 3 (Page 29), when combining different polynomials to form $t(X)$, why are different powers of $\alpha$ i.e. $ \alpha^0, \alpha^1, \alpha^2$ used.

**A:** Let's say we have 4 polynomials - $f_1$, $f_2$, $f_3$ & $f_4 \in \mathbb F_p[x]$ where the max degree of these polynomials is $d$ which is very, very small as compared to $p$.

We want to combine them into one polynomial $f$ such that if $f$ is 0 at some point, then all of $f_i$'s are also zero at the same point.

Consider the set ${1, z, z^2, z^3}$
This is a linearly independent set.

We can use this set to combine four variables $a_1, a_2, a_3$ & $a_4$ like this

$g(z) = a1 + a2\cdot z + a3\cdot z^2 + a4\cdot z^3$  

If $g(z)=0$ at some $z \ne 0$, then it means $a_1 = a_2 = a_3 = a_4 = 0$ (by the definition of a linearly independent set)

So we can combine the polynomials $f_i$'s as

$f(x,z) = f1(x) + z.f2(x) + z^2.f3(x) + z^3.f4(x)$

At some  $x = r_1$, let 

$f_1(r_1) = a_1$, $f_2(r_1) = a_2$, $f_3(r_1) = a_3$, $f_4(r_1) = a_4$,

So now

$f(x=r_1, z) = a1 + a2\cdot z + a3\cdot z^2 + a4\cdot z^3$

At some random value $r_2$ chosen from $\mathbb F_p$, if 

$f(x=r_1, z = r_2) = 0$   

then it means  $a1 = a2 = a3 = a4 = 0$ 

i.e. $f_1(r_1, r_2) =f_2(r_1, r_2) = f_3(r_1, r_2) =  f_4(r_1, r_2) = 0$

If $f_1$, $f_2$, $f_3$, $f_4$ are all $0$ at some random value $r_2$, then by the Schwartz-Zippel lemma, $f_1$, $f_2$, $f_3$, $f_4$ are all zero polynomials with very high probability because the maximum degree of these polynomials is very, very small as compared to $p$

So if we want to test if multiple polynomials are zero polynomials or not, we can combine them thus & test them with one evaluation at a random point rather than testing them separately using this technique. Round 3 in the PLONK paper creates the polynomial $t(X)$ with the linearly independent set $[1, \alpha, \alpha^2]$ to do this.
 
---- 
#### Multiplicative Subgroup
**Q:** Why does PLONK use a multiplicative subgroup?

**A:** There are multiple reasons why PLONK uses a multiplicative subgroup

$(1)\space$ [Every element of a finite field is a root of unity](/WeilMOV#roots-of-unity). A Finite Field $\mathbb F_p$ has a multiplicative subgroup of order $k$ only if $k$ divides $p-1$. All primitive roots of unity in a finite field also form a multiplicative subgroup of the field. Let $\omega$ be a primitive $k$th root of unity in $\mathbb F_p$ i.e. $\omega^k = 1$ This forms a multiplicative subgroup of order $k$ - let's call it $H$

$H = \lbrace \omega, \omega^2, \omega^3, ..., \omega^{k-1}, 1 \rbrace$ 
    
(the last element $\omega^k$ is equal to $1$)

In [Groth16](/R1CSQAP/), we compute the vanishing polynomial 

$Z_H = (x-1)\star(x-2)\star(x-2)\star ...\star(x-k)$ (where 1, 2, 3 etc are the gate numbers)

The number of gates is usually very large (may be a million gates or more). So computing the vanishing polynomial which has a million such terms is quite expensive.

PLONK numbers the gates using the elements of the multiplicative subgroup $H$. Now the vanishing polynomial becomes  

$Z_H = (x-\omega)\star (x-\omega^2)\star (x-\omega^3)\star ...\star (x-1)$

Now,

Let's consider the polynomial $x^k - 1$

- For $x = \omega$, 

    Since $\omega$ is the $k$th root of unity, $x^k = 1$ & hence $\omega$ is a root of $x^k - 1$

- For $x = \omega^2$ 

  ${\omega^2}^k = {\omega^k}^2 = {1}^2 = 1$.

  So $\omega^2$ is also a root of $x^k - 1$

- Like this, we can prove that every element of $H$ is a root of $x^k - 1$ & since $x^k - 1$ is degree $k$, the maximum number of roots it can have is $k$.
  
So $x^k - 1 = (x-\omega)\star (x-\omega^2)\star (x-\omega^3), ..., (x-1)$

  So $Z_H = x^k - 1$

  So now the vanishing polynomial $Z_H$ is very easy to compute instead of having to multiply a million terms.


$(2)$ Using a multiplicative subgroups allows an efficient & elegant Product Check on the subgroup.

With a polynomial $M(x)$, the prover has to prove that 

$\prod_{x\in H} M(x)  = 1$


i.e the Prover has to prove that


$M(\omega) \star M(\omega^2) \star \cdots \star M(\omega^{k-1})\star M(1) = 1$

Let's define another polynomial $R(x)$ such that  

- $R(\omega) = 1$

- $R(x\omega) = R(x) \star M(x)$

Considering the above definition of $R$,

For $x=\omega$, $R(\omega^2) = $R(\omega\star\omega) = R(\omega)\star M(\omega) = 1 * M(\omega) = M(\omega)$

For $x=\omega^2$, $R(\omega^3) = $R(\omega^2\star\omega) = R(\omega^2)\star M(\omega^2) = M(\omega)*M(\omega^2)$

For $x = \omega^3$, $R(\omega^4) = M(\omega)\star M(\omega^2) \star M(\omega^3)$

$\cdots$

$\cdots$

With $ x = \omega^{k-1}$, $R(\omega^k) = M(\omega)\star M(\omega^2)\star \cdots \star M(\omega^{k-1})$

With $x = \omega^k$, $R(\omega) = M(\omega)\star M(\omega^2)\star \cdots\star M(\omega^k)$

Since $\omega^k = 1$, $R(\omega) = M(\omega)\star M(\omega^2)\star \cdots\star M(1)$

But we started with the assumption that $R(\omega) = 1$. Sp proving that $M(\omega) \star M(\omega^2) \star \cdots \star M(\omega^{k-1})\star M(1) = 1$ (i.e. proving that $\prod_{x\in H} M(x)  = 1$)

would consist of just 

1) Proving $R(\omega) = 1$

2) Proving $R(\omega^{k+1})$ was built accumulatively from the earlier $R$'s

which can be done elegantly & efficiently as described in that section of the PLONK paper.

This proof is based on our definition of $R(x)$

$R(x\omega) = R(x) \star M(x)$

We can define $R$ so because we are operating in a multiplicative subgroup 
$H = \lbrace \omega, \omega^2, \omega^3, ..., \omega^{k-1}, 1 \rbrace$, where multiplying by each element  $\omega$ gives us the next element & thus we get the "right shift" relation between $R$ at an element & $R$ at the next element.

Section $5$ of the PLONK Paper describes "Polynomial protocols for identifying permutations" which uses the above product check $M(x) = \frac{f'(x)}{g'(x)}$ to prove that $\prod_{x\in H} \frac{f'(x)}{g'(x)}  = 1$

[![Hits](https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Frisencrypto.github.io%2FPLONKWHY%2F&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=hits&edge_flat=false)](https://hits.seeyoufarm.com)
